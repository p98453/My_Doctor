# MyDoctor
# 前言

该项目是一个依托Django服务器为调度中心，链接客户端、MySQL数据库端、大模型端以及RAG系统端的全栈部署，后续开发可以依据这个部署思路实现一个可控，可追溯，低幻觉的大模型对话平台。

当然这里只是用作学习，整个系统保留躯干部分

# 一、项目整体架构

## 1.1 技术栈

**Qwen-7B（基座模型） + LLaMA-Factory + QLoRA + vLLM + GraphRAG + Milvus + Sentence Transformers + Django + MySQL + uniApp + Langchain + HuggingFace**

1. **Qwen-7B**：核心大语言模型
2. **uniApp**：前端交互
3. **Django**：后端服务
4. **GraphRAG**：知识图谱检索
5. **Milvus**：向量数据库
6. **vLLM**：高效推理部署
7. **MySQL**：日志和历史存储
8. **Sentence Transformers**：文本向量化
9. **LLaMA-Factory**：模型管理
10. **QLoRA**：低秩微调
11. **Langchain / HuggingFace**：链式调用和接口管理

## 1.2 架构图

![整体架构](README/架构图.png)



------

# 二、系统链路说明

## 1. 用户输入问题

![image-20250916104544915](README/image-20250916104544915.png)

1. 用户在客户端输入自然语言问题，例如：

```
小米投资了哪些公司？。
```

------

## 2. 服务器接收与调度

![image-20250916104324985](README/image-20250916104324985.png)

- 服务器接收到用户问题后，将问题转发给 **RAG 模块** 进行知识检索。
- 系统支持三种知识检索方式：
  1. 知识块文本直接检索
  2. 词向量数据库检索
  3. **GraphRAG**（语义 + 实体关系网络 + 推理路径）

> 此处选用 **GraphRAG**，因为它具有最强的精确性和可解释性。

------

## 3. 上下文检索阶段（以 Neo4j 知识图谱为核心）★★★

![image-20250916104436495](README/image-20250916104436495.png)

RAG 不仅依赖向量相似度，更通过结构化语义关系生成上下文信息。

### 3.1 问题解析

- 对用户输入进行 **命名实体识别（NER）** 和 **关系抽取**
- 示例：“小米投资了哪些公司”
  - 实体：`小米`
  - 关系：`投资`
  - 目标实体类型：`公司`

### 3.2 图谱遍历查询（GraphRAG）

- 根据识别出的实体、关系和目标类型，动态生成 Cypher 查询语句，并在 Neo4j 中检索相关节点和关系
- 查询结果可包含直接匹配节点以及上下游相关事件，例如：

```cypher
MATCH (i:Investor)-[:INVEST]->(c:Company)
WHERE i.name CONTAINS "小米"
OPTIONAL MATCH (c)-[r:HAPPEN]->(e:EventType)
RETURN i.name as investor, c.name as company_name, e.name as event_type
```

- 将查询结果按照 **投资人 → 公司 → 事件** 层级遍历，生成可读文本：

```
小米投资了字节跳动，该公司发生了以下事件：融资、上市……
小米投资了优酷，该公司发生了以下事件：战略合作……
```

### 3.3 上下文构建（结合词向量数据库）

流程如下：

1. **结构化信息遍历**
   - 遍历 Neo4j 查询结果的实体-关系-目标映射，生成可读文本片段（chunk）。
2. **向量化存储**
   - 将文本片段存入 **词向量数据库**（Milvus），计算并存储语义向量。
3. **向量相关性检索**
   - 使用用户问题的语义向量，从数据库检索最相关的文本片段，剔除低相关内容。
4. **二次精排（Rerank）**
   - 候选片段输入 rerank 模型，结合问题语义和上下文逻辑排序，输出高相关性且有序的上下文列表。
5. **上下文输出**
   - 排序后的文本片段作为 RAG 上下文**返回服务器**，为大模型提供输入依据。

------

## 4. 大模型推理阶段（Prompt Engineering + vLLM）

![image-20250916104505434](README/image-20250916104505434.png)

### 4.1 构建 Prompt

服务器将用户原始问题与 Neo4j 检索到的上下文拼接成 Prompt，例如：

```
上下文信息：
1. 小米投资了字节跳动，该公司发生了融资事件。
2. 小米投资了优酷，该公司发生了战略合作事件。

问题：
小梅想了解小米投资了哪些公司以及相关事件，请给出详细回答。

要求：
- 条理清晰，分点列出
- 尽量使用上下文中的信息
- 对未提及的信息可标注“未找到”
```

### 4.2 模型推理

- Prompt 输入 **vLLM** 模型生成答案，例如：

> 小米已投资多家企业，包括字节跳动、优酷、蔚来汽车等，覆盖互联网、媒体和新能源汽车等领域。

### 4.3 结果返回与存储

![image-20250916104603053](README/image-20250916104603053.png)

- **结果返回**
  - 服务器将生成答案打包为 Response 返回客户端，用户可即时查看。
- **数据存储**
  - 服务器将以下内容存入 **MySQL**：
    - 用户的提问（原始问题）
    - 大模型生成的回答
  - 支持多轮对话和后续分析。

## 三、搭建血泪史

渲染数据

![image-20250907122001283](README/image-20250907122001283.png)

调用api回应请求

![image-20250907131600355](README/image-20250907131600355.png)

构建服务器

![image-20250907163209894](README/image-20250907163209894.png)

功能模块创建

![image-20250907163325029](README/image-20250907163325029.png)

![image-20250907164533148](README/image-20250907164533148.png)

启动服务器

![image-20250907164808358](README/image-20250907164808358.png)

将前段端口从大模型改到服务器，解决**跨域问题**，实现前后端交互

![image-20250910183856872](README/image-20250910183856872.png)

![image-20250910184311568](README/image-20250910184311568.png)

服务器接收前段输入

![image-20250910192547084](README/image-20250910192547084.png)

流式输出

![image-20250911151136996](README/image-20250911151136996.png)

数据库创建表

![image-20250911153656091](README/image-20250911153656091.png)

生成迁移文件

![image-20250911155302990](README/image-20250911155302990.png)

迁移到数据库中

![image-20250911155436455](README/image-20250911155436455.png)

服务器存储前端输入内容存入服务器

![](README/image-20250911183655087.png)

将每一轮对话设计成一个相同的topic_id,方便后面进行上下文检索

![image-20250911192201533](README/image-20250911192201533.png)

存储模型响应结果

![image-20250911195044752](README/image-20250911195044752.png)

显示对话列表，并转到相应对话，后续可以进行上下文互通，多轮对话

![image-20250912101754819](README/image-20250912101754819.png)

多轮对话，(system prompt + 历史对话 + 本次用户输入 + 模型生成输出) ≤ 10000 tokens（--max-model-len 10000）

![image-20250912104626047](README/image-20250912104626047.png)

管理上下文总tokens

![image-20250912112937300](README/image-20250912112937300.png)

调用大模型计算总的tokens，并删除超出的内容

![image-20250912193517262](README/image-20250912193517262.png)

导入全局变量

![image-20250912195156306](README/image-20250912195156306.png)

保存前端提交的文件

![image-20250913104539088](README/image-20250913104539088.png)

将文件内容交给大模型，实现文件交互

![image-20250913104304022](README/image-20250913104304022.png)

### RAG

使用langchain读取不同格式的文档信息

### 首先安装依赖包

pip install langchain-community
pip install langchain-unstructured

### 读取目录中的非结构化数据

pip install unstructured
pip install "unstructured[pptx]"
pip install "unstructured[image]"
pip install "unstructured[md]"
pip install "unstructured[pdf]"

pip install "unstructured[xlsx]"

### 识别图片

conda install -c conda-forge libmagic

下载软件包和语言包（指定文件存放），配置相关环境变量

![image-20250913132717434](README/image-20250913132717434.png)

![image-20250913132928836](README/image-20250913132928836.png)

### 将目录下面的文件全部读取，并存储为Json文件

![image-20250913143545894](README/image-20250913143545894.png)

### 语义分割

基于某一个**语义分割大模型**完成语义分割，实现知识点分块，减小上下文腐蚀

![image-20250913152542630](README/image-20250913152542630.png)

把分割后的知识库保存到新的json文件，每个知识块作为一个独立的条目

![image-20250913155521098](README/image-20250913155521098.png)

基于**rerank模型**，计算每个知识块与问题的文本相关性分数，以此决定哪些文本需要进入上下文

但是我们**通常不直接这么添加上下文**，详细内容接着往下看

![image-20250913163401024](README/image-20250913163401024.png)

### 词向量数据库

pip install chromadb

chroma run --path RAG检索增强生成/chromadb --host 127.0.0.1 --port 10221

激活服务器，将chroma 词向量存储在./chromadb下![image-20250913232719684](README/image-20250913232719684.png)

将知识块用语义分割模型分割成知识块后，使用嵌入向量生成模型为每个知识块生成向量，存到向量数据库中

当我们用collection.query来查询用户输入问题与向量数据库存储的知识块的相关性，嵌入向量生成模型将问题转化为词向量，在数据库中寻找相似度最近的返回

![image-20250914142608639](README/image-20250914142608639.png)

使用rerank模型进行二次精排

